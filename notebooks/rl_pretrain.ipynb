{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/network/scratch/a/aghajohm/hf_home\" # set before transformers\n",
    "os.environ[\"CUDA_HOME\"] = \"/cvmfs/ai.mila.quebec/apps/arch/common/cuda/12.5.1\" # Hardcoded for now\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import sglang\n",
    "import sys\n",
    "# Add the parent directory to the path so we can import from aha.py\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from aha import initialize_model, generate_r1_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response):\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    # Format the response with syntax highlighting\n",
    "    formatted_html = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; border: 1px solid #ddd;\">\n",
    "        <h3 style=\"color: #333; margin-top: 0;\">Generated Response:</h3>\n",
    "        <pre style=\"background-color: #f5f5f5; padding: 10px; border-radius: 3px; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\">{response}</pre>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    return HTML(formatted_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_OR_NAME = '/network/scratch/a/aghajohm/aha_models/test_checkpoint'\n",
    "CHAT_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" # should have the tokenizer we trained the checkpoint with\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_NAME)\n",
    "CHECKPOINT_OR_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_r1_prompt():\n",
    "    r1_prefix = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"What is gravity?\",\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"Gravity\"},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        r1_prefix, tokenize=True, continue_final_message=True\n",
    "    )\n",
    "    prompt = tokenizer.decode(\n",
    "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"input_ids\": input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_r1_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sglang_engine = sglang.Engine(\n",
    "        model_path=CHECKPOINT_OR_NAME,\n",
    "        enable_memory_saver=True,\n",
    "        skip_tokenizer_init=True,\n",
    "        mem_fraction_static=0.20,\n",
    "        schedule_policy=\"fcfs\",\n",
    "        schedule_conservativeness=0.001,\n",
    "        max_running_requests=10000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_CHECKPOINT_OR_NAME = \"Qwen/Qwen2.5-1.5B\"\n",
    "pretrained_tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_CHECKPOINT_OR_NAME)\n",
    "pretrained_sglang_engine = sglang.Engine(\n",
    "    model_path=PRETRAINED_CHECKPOINT_OR_NAME,\n",
    "    enable_memory_saver=True,\n",
    "    skip_tokenizer_init=True,\n",
    "    mem_fraction_static=0.20,\n",
    "    schedule_policy=\"fcfs\",\n",
    "    schedule_conservativeness=0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "full_text = \"In physics, gravity (from Latin gravitas 'weight'[1]) is a fundamental interaction primarily observed as a mutual attraction between all things that have mass. Gravity is, by far, the weakest of the four fundamental interactions, approximately 1038 times weaker than the strong interaction, 1036 times weaker than the electromagnetic force, and 1029 times weaker than the weak interaction. As a result, it has no significant influence at the level of subatomic particles.[2] However, gravity is the most significant interaction between objects at the macroscopic scale, and it determines the motion of planets, stars, galaxies, and even light.\"\n",
    "random_cutoff = random.randint(0, len(full_text) - 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"random cutoff: {random_cutoff}\")\n",
    "text_to_complete = full_text[:random_cutoff]\n",
    "ground_truth_completion = full_text[random_cutoff:]\n",
    "text_tokenized = pretrained_tokenizer(text_to_complete)\n",
    "pretrain_sampling_params = {\n",
    "        \"temperature\": 1.0,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"top_p\": 1.0,\n",
    "        \"n\": 1,  # Only generate one completion per text\n",
    "    }\n",
    "    \n",
    "generation = pretrained_sglang_engine.generate(input_ids=text_tokenized[\"input_ids\"], sampling_params=pretrain_sampling_params)\n",
    "response = tokenizer.decode(generation[\"output_ids\"])\n",
    "pretrained_completion = response\n",
    "format_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer. Always announce your score in \\\\score{} like this: \\\\score{your_score}\\n\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Your task is to compare A and B. \"\\\n",
    "                        \"Give the completion a score from 0 to 100 based on how closely B matches A in terms of content. It can have a different style, but the content should be the same. \"\\\n",
    "                        \"Always announce your score in \\\\score{} like this: \\\\score{your_score}\\n\"\\\n",
    "                       f\"A:\\n{ground_truth_completion}\\n\"\\\n",
    "                       f\"B:\\n{pretrained_completion}\\n\",\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's compare B against A.<think>\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.apply_chat_template(chat, tokenize=True, continue_final_message=True)\n",
    "prompt = tokenizer.decode(input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "chat_tokenized = {\"prompt\": prompt, \"input_ids\": input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(response):\n",
    "    import re\n",
    "    \n",
    "    # Try to extract score from \\score{} format\n",
    "    score_pattern = r'\\\\score\\{(\\d+)\\}'\n",
    "    score_match = re.search(score_pattern, response)\n",
    "    if score_match:\n",
    "        try:\n",
    "            return int(score_match.group(1))\n",
    "        except (ValueError, IndexError):\n",
    "            return None\n",
    "        \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_sampling_params = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 1024*4,\n",
    "        \"top_p\": 1.0,\n",
    "        \"n\": 40,  # Only generate one response per question\n",
    "    }\n",
    "    \n",
    "generations = sglang_engine.generate(input_ids=chat_tokenized[\"input_ids\"], sampling_params=eval_sampling_params)\n",
    "scores = []\n",
    "for generation in generations:\n",
    "    response = tokenizer.decode(generation[\"output_ids\"])\n",
    "    scores.append(extract_score(response)) \n",
    "\n",
    "import numpy as mean\n",
    "print(scores)\n",
    "scores = [score for score in scores if score is not None]\n",
    "if len(scores) > 0:\n",
    "    print(f\"mean score: {np.mean(scores)}, std score: {np.std(scores)}\")\n",
    "else:\n",
    "    print(\"No valid scores found\")\n",
    "\n",
    "# plot the scores\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(scores, bins=20, range=(0, 100), density=True)\n",
    "plt.show()\n",
    "format_response(response)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_response(tokenizer.decode(generations[-2][\"output_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_score(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_completion(ground_truth_completion, pretrained_completion):\n",
    "    chat = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer. Always announce your score in \\\\score{} like this: \\\\score{your_score}\\n\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Your task is to compare A and B. \"\\\n",
    "                        \"Give the completion a score from 0 to 100 based on how closely B matches A in terms of content. It can have a different style, but the content should be the same. \"\\\n",
    "                        \"Always announce your score in \\\\score{} like this: \\\\score{your_score}\\n\"\\\n",
    "                       f\"A:\\n{ground_truth_completion}\\n\"\\\n",
    "                       f\"B:\\n{pretrained_completion}\\n\",\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": \"Let's compare B against A.<think>\"},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(chat, tokenize=True, continue_final_message=True)\n",
    "    prompt = tokenizer.decode(input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    chat_tokenized = {\"prompt\": prompt, \"input_ids\": input_ids}\n",
    "    generations = sglang_engine.generate(input_ids=chat_tokenized[\"input_ids\"], sampling_params=eval_sampling_params)\n",
    "    scores = []\n",
    "    for generation in generations:\n",
    "        response = tokenizer.decode(generation[\"output_ids\"])\n",
    "        scores.append(extract_score(response))\n",
    "    scores = [score for score in scores if score is not None]\n",
    "    if len(scores) > 0:\n",
    "        return np.mean(scores), np.std(scores)\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "full_text = \"In physics, gravity (from Latin gravitas 'weight'[1]) is a fundamental interaction primarily observed as a mutual attraction between all things that have mass. Gravity is, by far, the weakest of the four fundamental interactions, approximately 1038 times weaker than the strong interaction, 1036 times weaker than the electromagnetic force, and 1029 times weaker than the weak interaction. As a result, it has no significant influence at the level of subatomic particles.[2] However, gravity is the most significant interaction between objects at the macroscopic scale, and it determines the motion of planets, stars, galaxies, and even light.\"\n",
    "for cutoff in range(50, len(full_text), 50):\n",
    "    print(f\"random cutoff: {cutoff}\")\n",
    "    for try_num in range(3):\n",
    "        print(f\"try_num: {try_num}\")\n",
    "        text_to_complete = full_text[:cutoff]\n",
    "        ground_truth_completion = full_text[cutoff:]\n",
    "        text_tokenized = pretrained_tokenizer(text_to_complete)\n",
    "        pretrain_sampling_params = {\n",
    "                \"temperature\": 1.0,\n",
    "                \"max_new_tokens\": 1024,\n",
    "                \"top_p\": 1.0,\n",
    "                \"n\": 1,  # Only generate one completion per text\n",
    "            }\n",
    "            \n",
    "        generation = pretrained_sglang_engine.generate(input_ids=text_tokenized[\"input_ids\"], sampling_params=pretrain_sampling_params)\n",
    "        response = tokenizer.decode(generation[\"output_ids\"])\n",
    "        pretrained_completion = response\n",
    "        mean_score, std_score = rank_completion(ground_truth_completion, pretrained_completion)\n",
    "        print(f\"mean score: {mean_score}, std score: {std_score}\", end='\\t')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = \"/network/scratch/a/aghajohm/hf_home\" # set before transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import sglang\n",
    "import sys\n",
    "# Add the parent directory to the path so we can import from aha.py\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from aha import initialize_model, generate_r1_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_response(response):\n",
    "    from IPython.display import HTML\n",
    "\n",
    "    # Format the response with syntax highlighting\n",
    "    formatted_html = f\"\"\"\n",
    "    <div style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; border: 1px solid #ddd;\">\n",
    "        <h3 style=\"color: #333; margin-top: 0;\">Generated Response:</h3>\n",
    "        <pre style=\"background-color: #f5f5f5; padding: 10px; border-radius: 3px; overflow-x: auto; white-space: pre-wrap; word-wrap: break-word;\">{response}</pre>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    return HTML(formatted_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_OR_NAME = '/network/scratch/a/aghajohm/aha_models/test_checkpoint'\n",
    "CHAT_MODEL_NAME = \"Qwen/Qwen2.5-3B-Instruct\" # should have the tokenizer we trained the checkpoint with\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_NAME)\n",
    "# CHECKPOINT_OR_NAME = \"Qwen/Qwen2.5-3B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sglang_engine = sglang.Engine(\n",
    "        model_path=CHECKPOINT_OR_NAME,\n",
    "        enable_memory_saver=True,\n",
    "        skip_tokenizer_init=True,\n",
    "        mem_fraction_static=0.20,\n",
    "        schedule_policy=\"fcfs\",\n",
    "        schedule_conservativeness=0.001,\n",
    "        max_running_requests=10000,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# play the countdown game\n",
    "numbers = [1, 2, 3, 4]\n",
    "target = 10\n",
    "prompt = generate_r1_prompt(numbers, target, tokenizer)\n",
    "\n",
    "eval_sampling_params = {\n",
    "        \"temperature\": 0.3,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"top_p\": 1.0,\n",
    "        \"n\": 1,  # Only generate one response per question\n",
    "    }\n",
    "    \n",
    "generation = sglang_engine.generate(input_ids=prompt[\"input_ids\"], sampling_params=eval_sampling_params)\n",
    "response = tokenizer.decode(generation[\"token_ids\"])\n",
    "format_response(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general chat with the model\n",
    "def generate_chat_prompt(query, tokenizer):\n",
    "    r1_prefix = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
    "      },\n",
    "      { \n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{query}\"\n",
    "      }]\n",
    "    input_ids = tokenizer.apply_chat_template(r1_prefix, tokenize=True, continue_final_message=False)\n",
    "    prompt = tokenizer.decode(input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False)\n",
    "    return {\"prompt\": prompt, \"input_ids\": input_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Hello the mirror on the wall, who is the best one of all?\"\n",
    "\n",
    "chat_prompt = generate_chat_prompt(user_query, tokenizer)\n",
    "\n",
    "eval_sampling_params = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"top_p\": 1.0,\n",
    "        \"n\": 1,  # Only generate one response per question\n",
    "    }\n",
    "\n",
    "chat_response = sglang_engine.generate(input_ids=chat_prompt[\"input_ids\"], sampling_params=eval_sampling_params)\n",
    "chat_response = tokenizer.decode(chat_response[\"token_ids\"])\n",
    "format_response(chat_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = initialize_model(CHECKPOINT_OR_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 fixed modules",
   "language": "python",
   "name": "python3_fixed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

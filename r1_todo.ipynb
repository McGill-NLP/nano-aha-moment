{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimalist Reproduction of R1-Zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "SCRATCH = Path.home() / \"scratch\"\n",
    "\n",
    "os.environ[\"HF_HOME\"] = str(SCRATCH / \"hf_home\")\n",
    "# os.environ[\"CUDA_HOME\"] = \"/cvmfs/ai.mila.quebec/apps/arch/common/cuda/12.5.1\" # Hardcoded for now\n",
    "# os.environ[\"HF_TOKEN\"] = \"...\" # Optional. Only needed for Llama models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import re\n",
    "import socket\n",
    "import time\n",
    "import gc\n",
    "from typing import List\n",
    "from typing import Tuple\n",
    "\n",
    "import deepspeed\n",
    "import numpy as np\n",
    "import sglang\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from tqdm import trange\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from typing import Union\n",
    "from typing import Any, Dict\n",
    "\n",
    "from deepspeed import DeepSpeedEngine\n",
    "from transformers import PreTrainedModel\n",
    "\n",
    "from utils import compute_token_log_probs, prepare_model_inputs, evaluate_on_test_set, find_free_port\n",
    "from test_utils import compute_reward_ground, create_training_episodes_ground, compute_pg_loss_ground\n",
    "\n",
    "# Needed to stop DeepSpeed from complaining\n",
    "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "os.environ[\"MASTER_PORT\"] = str(find_free_port())\n",
    "os.environ[\"RANK\"] = \"0\"\n",
    "os.environ[\"LOCAL_RANK\"] = \"0\"\n",
    "os.environ[\"WORLD_SIZE\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-3B\"\n",
    "MODEL_CHAT_NAME = MODEL_NAME + \"-Instruct\"\n",
    "\n",
    "# RL parameters\n",
    "NUM_ITERATIONS = 1000\n",
    "EPISODES_PER_ITERATION = 64\n",
    "GENERATIONS_PER_SAMPLE = 4\n",
    "KL_COEFFICIENT = 0.001\n",
    "\n",
    "# Training hyperparameters\n",
    "PER_DEVICE_BATCH_SIZE = 4\n",
    "LEARNING_RATE = 1e-6\n",
    "\n",
    "# Sampling parameters\n",
    "MAX_RESPONSE_TOKENS = 1024\n",
    "TEMPERATURE = 1.0\n",
    "TOP_P = 1.0\n",
    "TOP_K = -1 # no top k\n",
    "\n",
    "# DeepSpeed configuration\n",
    "deepspeed_config = {\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\"stage\": 2, \"overlap_comm\": False},\n",
    "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
    "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_clipping\": 1.0,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": LEARNING_RATE,\n",
    "            \"betas\": (0.9, 0.999),\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 0.0,\n",
    "            \"torch_adam\": True,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "ref_deepspeed_config = {\n",
    "    \"bf16\": {\"enabled\": True},\n",
    "    \"train_batch_size\": EPISODES_PER_ITERATION,\n",
    "    \"train_micro_batch_size_per_gpu\": PER_DEVICE_BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": EPISODES_PER_ITERATION // PER_DEVICE_BATCH_SIZE,\n",
    "}\n",
    "\n",
    "RUN_NAME = \"r1-zero\"\n",
    "EXP_DIR = SCRATCH / \"deepseek_hackathon\" / RUN_NAME\n",
    "EXP_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MESSAGE = \"You are a helpful assistant. You first thinks about the reasoning process in the mind and then provides the user with the answer.\"\n",
    "PROMPT_TEMPLATE = \"Using the numbers {numbers}, create an equation that equals {target}. You can use basic arithmetic operations (+, -, *, /) and each number can only be used once. Show your work in <think> </think> tags. And return the final equation and answer in <answer> </answer> tags, for example <answer>(1 + 2) / (3 * 5)</answer>.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process dataset\n",
    "def preprocess_example(example: Dict[str, Any]):\n",
    "    numbers: List[int] = example[\"nums\"]\n",
    "    target: int = example[\"target\"]\n",
    "\n",
    "    prefix = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_MESSAGE},\n",
    "        {\"role\": \"user\", \"content\": PROMPT_TEMPLATE.format(numbers=numbers, target=target)},\n",
    "        {\"role\": \"assistant\", \"content\": \"Let me solve this step by step.\\n<think>\"},\n",
    "    ]\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        prefix, tokenize=True, continue_final_message=True\n",
    "    )\n",
    "    prompt = tokenizer.decode(\n",
    "        input_ids, skip_special_tokens=False, clean_up_tokenization_spaces=False\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"input_ids\": input_ids}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHAT_NAME)\n",
    "EOS_TOKEN_ID = AutoTokenizer.from_pretrained(MODEL_NAME).eos_token_id\n",
    "EOS_TOKEN = tokenizer.convert_ids_to_tokens(EOS_TOKEN_ID)\n",
    "\n",
    "dataset = load_dataset(\"Jiayi-Pan/Countdown-Tasks-3to4\", split=\"train\")\n",
    "dataset = dataset.map(preprocess_example, num_proc=6)\n",
    "\n",
    "# Split dataset\n",
    "train_test_split = dataset.train_test_split(test_size=500, seed=42)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "len(train_dataset), len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[0][\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset[2][\"prompt\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_reward_func(completion: str) -> float:\n",
    "    \"\"\"\n",
    "    Format: <think>...</think><answer>...</answer>\n",
    "\n",
    "    Also checks that the content within <answer>...</answer> conforms to a\n",
    "    specified pattern (only digits, + - * / ( ) . and whitespace).\n",
    "\n",
    "    Args:\n",
    "        completion (str): Generated output\n",
    "        EOS_TOKEN (str): End of sequence token\n",
    "\n",
    "    Returns:\n",
    "        float: Reward score\n",
    "    \"\"\"\n",
    "    # Define the allowed pattern (only numbers, +, -, *, /, (, ), ., and whitespace)\n",
    "    allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "\n",
    "    try:\n",
    "        # Synthetically prepend <think> (if your pipeline relies on that to ease matching)\n",
    "        completion = \"<think>\" + completion\n",
    "\n",
    "        # Strip EOS token if present\n",
    "        if completion.endswith(EOS_TOKEN):\n",
    "            completion = completion[:-len(EOS_TOKEN)]\n",
    "\n",
    "        # Check if the format is correct\n",
    "        # Pattern means:\n",
    "        # 1) <think>...contents not including other <think> tags...</think>\n",
    "        # 2) \\n\n",
    "        # 3) <answer>...anything...</answer>\n",
    "        regex = r\"^<think>([^<]*(?:<(?!/?think>)[^<]*)*)<\\/think>\\n<answer>([\\s\\S]*?)<\\/answer>$\"\n",
    "        match = re.search(regex, completion, re.DOTALL)\n",
    "\n",
    "        if match is None or len(match.groups()) != 2:\n",
    "            # Format is incorrect\n",
    "            return 0.0\n",
    "        else:\n",
    "            # Extract the content inside <answer>...</answer>\n",
    "            answer_content = match.group(2).strip()\n",
    "\n",
    "            # Check if answer content matches the allowed pattern\n",
    "            if not re.match(allowed_pattern, answer_content):\n",
    "                # If it doesn't match, reward is 0.5\n",
    "                return 0.5\n",
    "            else:\n",
    "                # If both format and pattern are correct, reward is 1\n",
    "                return 1.0\n",
    "    except Exception:\n",
    "        # Any error leads to 0 reward\n",
    "        return 0.0\n",
    "\n",
    "\n",
    "def equation_reward_func(completion: str, nums: List[int], target: int) -> float:\n",
    "    \"\"\"\n",
    "    Evaluates completion based on mathematical correctness of the answer\n",
    "\n",
    "    Args:\n",
    "        completion (str): Generated output\n",
    "        target (str): Expected answer\n",
    "        nums (list): Available numbers to use in the equation\n",
    "\n",
    "    Returns:\n",
    "        float: Reward score\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # add synthetic <think> as its already part of the prompt and prefilled for the assistant to more easily match the regex\n",
    "        completion = \"<think>\" + completion\n",
    "        # Check if the format is correct\n",
    "        match = re.search(r\"<answer>(.*?)<\\/answer>\", completion)\n",
    "        if match is None:\n",
    "            return 0.0\n",
    "        # Extract the \"answer\" part from the completion\n",
    "        equation = match.group(1).strip()\n",
    "        \n",
    "        # Extract all numbers and Check if all numbers are used exactly once\n",
    "        #TODO: I need your help here\n",
    "        # Define a regex pattern that only allows numbers, operators, parentheses, and whitespace\n",
    "        allowed_pattern = r\"^[\\d+\\-*/().\\s]+$\"\n",
    "        if not re.match(allowed_pattern, equation):\n",
    "            return 0.0\n",
    "\n",
    "        # Evaluate the equation with restricted globals and locals\n",
    "        result = eval(equation, {\"__builtins__\": None}, {})\n",
    "        # Check if the equation is correct and matches the ground truth\n",
    "        if abs(float(result) - float(target)) < 1e-5:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n",
    "    except Exception:\n",
    "        # If evaluation fails, reward is 0\n",
    "        return 0.0\n",
    "    \n",
    "\n",
    "def compute_reward(completion: str, sample: Dict[str, Any]) -> Tuple[float, Dict[str, float]]:\n",
    "    nums = sample[\"nums\"]\n",
    "    target = sample[\"target\"]\n",
    "\n",
    "    format_reward = format_reward_func(completion)\n",
    "    equation_reward = equation_reward_func(\n",
    "        completion=completion, nums=nums, target=target\n",
    "    )\n",
    "\n",
    "    reward = format_reward + equation_reward\n",
    "\n",
    "    metrics = {\n",
    "        \"format_reward\": format_reward,\n",
    "        \"equation_reward\": equation_reward,\n",
    "    }   \n",
    "\n",
    "    return reward, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_test_compute_reward():     \n",
    "    test_cases = [\n",
    "        (\"hello</think>\\n<answer>1+2+3+4</answer>\", {\"nums\": [1, 2, 3, 4], \"target\": 10}),\n",
    "        (\"<think>hello</think>\\n<answer>1+2+3+4</answer>\", {\"nums\": [1, 2, 3, 4], \"target\": 10}),\n",
    "        (\"hello</think>\\n<answer>1+2+3+3</answer>\", {\"nums\": [1, 2, 3], \"target\": 9}),\n",
    "        (\"hello</think>\\n<answer>5+6+4+3-1</answer>\", {\"nums\": [1, 3, 4, 6], \"target\": 17}),\n",
    "        (\"hello</think>\\n<answer>(3-1)*(9+7)</answer>\", {\"nums\": [1, 3, 9, 7], \"target\": 32}),\n",
    "    ]\n",
    "    \n",
    "    for completion, sample in test_cases:\n",
    "        out_ground = compute_reward_ground(completion, sample, EOS_TOKEN)\n",
    "        out = compute_reward(completion, sample)\n",
    "        assert out_ground == out\n",
    "\n",
    "unit_test_compute_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_episodes(\n",
    "    samples: List[Dict[str, Any]],\n",
    "    all_generations: List[List[int]],\n",
    "    all_finish_reasons: List[str],\n",
    ") -> Tuple[Dict[str, Any], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Process model generations and calculate rewards for training episodes.\n",
    "\n",
    "    This function processes generated responses and calculates rewards for training episodes by:\n",
    "    1. Grouping generations by sample (GENERATIONS_PER_SAMPLE responses per input)\n",
    "    2. Computing rewards and advantages for each response\n",
    "    3. Processing response tokens (adding EOS tokens where needed)\n",
    "\n",
    "    Args:\n",
    "        samples: List of input samples, each containing:\n",
    "            - input_ids: List[int], tokenized input prompt\n",
    "            - nums: List[int], numbers to use in equation\n",
    "            - target: int, target value for equation\n",
    "        all_generations: List of token ID sequences for each generated response\n",
    "        all_finish_reasons: List of finish reasons for each generation (\"stop\" or other)\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        1. Dictionary with processed data for training:\n",
    "            - all_query_token_ids: List[List[int]], input token IDs repeated for each generation\n",
    "            - all_response_token_ids: List[List[int]], response token IDs with EOS tokens added\n",
    "            - all_advantages: List[List[float]], advantage values repeated for each token\n",
    "        2. Dictionary with generation statistics:\n",
    "            - response_lengths: List[int], lengths of generated responses\n",
    "            - rewards: List[float], raw reward values\n",
    "            - non_stop_rate: List[bool], whether each generation ended naturally\n",
    "            - reward_metrics/*: Various reward component metrics\n",
    "\n",
    "    Example:\n",
    "        >>> samples = [{\"input_ids\": [1,2,3], \"nums\": [1,2,3], \"target\": 6}]\n",
    "        >>> generations = [[4,5], [6,7], [8,9]]  # 3 generations per sample\n",
    "        >>> finish_reasons = [\"stop\", \"length\", \"stop\"]\n",
    "        >>> episodes, stats = create_training_episodes(samples, generations, finish_reasons)\n",
    "        >>> episodes\n",
    "        {\n",
    "            'all_query_token_ids': [[1,2,3], [1,2,3], [1,2,3]],\n",
    "            'all_response_token_ids': [[4,5,EOS], [6,7], [8,9,EOS]],\n",
    "            'all_advantages': [[0.5,0.5,0.5], [-1.0,-1.0], [0.5,0.5,0.5]]\n",
    "        }\n",
    "    \"\"\"\n",
    "    assert len(all_generations) == len(all_finish_reasons)\n",
    "    assert len(all_generations) == len(samples) * GENERATIONS_PER_SAMPLE\n",
    "\n",
    "    # Process responses and calculate rewards\n",
    "    groups = [\n",
    "        list(range(i, i + GENERATIONS_PER_SAMPLE))\n",
    "        for i in range(0, len(all_generations), GENERATIONS_PER_SAMPLE)\n",
    "    ]  # example: [[0, 1, 2], [3, 4, 5], [6, 7, 8]]\n",
    "\n",
    "    all_query_token_ids, all_responses_token_ids, all_advantages = [], [], []\n",
    "\n",
    "    stats = {\n",
    "        \"response_lengths\": [],\n",
    "        \"rewards\": [],\n",
    "        \"non_stop_rate\": [],\n",
    "    }\n",
    "\n",
    "    for sample, group_indices in zip(samples, groups):\n",
    "        response_token_ids = [all_generations[i] for i in group_indices]\n",
    "        finish_reasons = [all_finish_reasons[i] for i in group_indices]\n",
    "        responses = tokenizer.batch_decode(\n",
    "            response_token_ids, skip_special_tokens=False\n",
    "        )\n",
    "        rewards_and_metrics = #TODO: I need your help here\n",
    "        rewards, reward_metrics = zip(*rewards_and_metrics)\n",
    "\n",
    "        rewards = np.array(rewards)\n",
    "        advantages = #TODO: I need your help here for GRPO advantages, (use 1e-4 as eps for denominator)\n",
    "\n",
    "        response_token_ids = [\n",
    "            (r + [EOS_TOKEN_ID]) if fr == \"stop\" else r\n",
    "            for r, fr in zip(response_token_ids, finish_reasons)\n",
    "        ]\n",
    "        \n",
    "        # note: they are all the same, as in GRPO the whole response is seen as a single action\n",
    "        per_token_advantages = [\n",
    "            [adv] * len(resp) for adv, resp in zip(advantages, response_token_ids)\n",
    "        ]\n",
    "\n",
    "        all_query_token_ids.extend([sample[\"input_ids\"]] * GENERATIONS_PER_SAMPLE)\n",
    "        all_responses_token_ids.extend(response_token_ids)\n",
    "        all_advantages.extend(per_token_advantages)\n",
    "\n",
    "        stats[\"rewards\"].extend(rewards)\n",
    "        stats[\"non_stop_rate\"].extend([fr != \"stop\" for fr in finish_reasons])\n",
    "        stats[\"response_lengths\"].extend([len(ids) for ids in response_token_ids])\n",
    "        for rm in reward_metrics:\n",
    "            for k, v in rm.items():\n",
    "                stats.setdefault(f\"reward_metrics/{k}\", []).append(v)\n",
    "\n",
    "    episodes = {\n",
    "        \"all_query_token_ids\": all_query_token_ids,\n",
    "        \"all_response_token_ids\": all_responses_token_ids,\n",
    "        \"all_advantages\": all_advantages,\n",
    "    }\n",
    "\n",
    "    return episodes, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_test_create_training_episodes():\n",
    "    test_cases = [\n",
    "        {\"sample\": {\"input_ids\": [1,2,3], \"nums\": [1,2,3], \"target\": 6},\n",
    "         \"generations\": [[4,5], [6,7], [8,9], [10,11]],\n",
    "         \"finish_reasons\": [\"stop\", \"length\", \"stop\", \"stop\"]},\n",
    "        \n",
    "        {\"sample\": {\"input_ids\": [33,44], \"nums\": [11, 7, 8], \"target\": 26},\n",
    "         \"generations\": [[1,2], [3,4], [5,6], [7,8]],\n",
    "         \"finish_reasons\": [\"stop\", \"stop\", \"length\", \"stop\"]},\n",
    "        \n",
    "        {\"sample\": {\"input_ids\": [9, 8, 7, 6, 5, 4], \"nums\": [1,2,3,4], \"target\": 10},\n",
    "         \"generations\": [[9,10], [11,12], [13,14], [15,16]],\n",
    "         \"finish_reasons\": [\"length\", \"length\", \"stop\", \"stop\"]}\n",
    "    ]\n",
    "    \n",
    "    for case in test_cases:\n",
    "        sample = case[\"sample\"]\n",
    "        generations = case[\"generations\"]\n",
    "        finish_reasons = case[\"finish_reasons\"]\n",
    "        \n",
    "        episodes_ground, stats_ground = create_training_episodes_ground([sample], generations, finish_reasons,\n",
    "                                                                        EOS_TOKEN, EOS_TOKEN_ID, GENERATIONS_PER_SAMPLE, tokenizer)\n",
    "        episodes, stats = create_training_episodes([sample], generations, finish_reasons)\n",
    "        \n",
    "        assert episodes_ground == episodes, f\"Mismatch in episodes for sample: {sample}\"\n",
    "        assert stats_ground == stats, f\"Mismatch in stats for sample: {sample}\"\n",
    "        \n",
    "unit_test_create_training_episodes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse KL Estimator\n",
    "\n",
    "One low variance estimator for KL divergence is given by:\n",
    "\n",
    "$$\n",
    "\\text{KL}[q, p] = (r - 1) - \\log r\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "r = \\frac{p(x)}{q(x)}\n",
    "$$\n",
    "\n",
    "with samples drawn from \\( q(x) \\).\n",
    "\n",
    "(taken from http://joschu.net/blog/kl-approx.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pg_loss(\n",
    "    policy_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
    "    reference_model: Union[DeepSpeedEngine, PreTrainedModel],\n",
    "    batch: Dict[str, torch.Tensor],\n",
    "    total_response_len: int,\n",
    ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Compute the policy gradient loss with KL penalty between policy and reference models.\n",
    "\n",
    "    This function:\n",
    "    1. Computes log probabilities for both policy and reference models\n",
    "    2. Calculates KL divergence penalty between the models\n",
    "    3. Computes policy gradient loss using advantages\n",
    "    4. Combines the losses with KL coefficient\n",
    "\n",
    "    Args:\n",
    "        policy_model: The model being trained\n",
    "        reference_model: The reference model for KL penalty calculation\n",
    "        batch: Dictionary containing:\n",
    "            - input_ids: Tensor of shape [batch_size, seq_len]\n",
    "            - attention_mask: Tensor of shape [batch_size, seq_len]\n",
    "            - labels: Tensor of shape [batch_size, seq_len] with -100 for ignored positions\n",
    "            - advantages: Tensor of shape [batch_size, seq_len]\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "            - loss: Combined policy gradient and KL penalty loss (scalar tensor)\n",
    "            - metrics: Dictionary with detailed loss components:\n",
    "                - policy_loss: Pure policy gradient loss\n",
    "                - kl_penalty: KL divergence penalty\n",
    "                - entropy: Policy entropy\n",
    "    \"\"\"\n",
    "    input_ids = batch[\"input_ids\"]  # [batch_size, seq_len]\n",
    "    attention_mask = batch[\"attention_mask\"]  # [batch_size, seq_len]\n",
    "    labels = batch[\"labels\"]  # [batch_size, seq_len]\n",
    "    advantages = batch[\"advantages\"]  # [batch_size, seq_len]\n",
    "\n",
    "    model_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "    labels_mask = (labels[..., 1:] != -100).float()  # [batch_size, seq_len-1]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_logps = compute_token_log_probs(\n",
    "            reference_model, model_inputs, TEMPERATURE\n",
    "        )  # [batch_size, seq_len-1]\n",
    "\n",
    "    logps = compute_token_log_probs(policy_model, model_inputs, TEMPERATURE)  # [batch_size, seq_len-1]\n",
    "\n",
    "    kl_penalty = #TODO: I need your help here (use the above kl estimator)  # [batch_size, seq_len-1]\n",
    "    kl_penalty = kl_penalty * labels_mask  # [batch_size, seq_len-1]\n",
    "\n",
    "    entropy = -logps.sum() / labels_mask.sum()  # scalar\n",
    "\n",
    "    policy_loss = #TODO: I need your help here (use the above kl estimator)  # [batch_size, seq_len-1]\n",
    "    policy_loss = policy_loss * labels_mask  # [batch_size, seq_len-1]\n",
    "\n",
    "    loss = (policy_loss + KL_COEFFICIENT * kl_penalty).sum() / total_response_len  # scalar\n",
    "\n",
    "    metrics = {\n",
    "        \"policy_loss\": policy_loss.sum().item() / total_response_len,\n",
    "        \"kl_penalty\": kl_penalty.sum().item() / total_response_len,\n",
    "        \"entropy\": entropy.item() / total_response_len,\n",
    "    }\n",
    "\n",
    "    return loss, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize main and reference models\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=0,\n",
    ")\n",
    "reference_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=0,\n",
    ")\n",
    "policy_model.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "# Initialize DeepSpeed engines\n",
    "policy_model, *_ = deepspeed.initialize(\n",
    "    model=policy_model,\n",
    "    config=deepspeed_config,\n",
    "    model_parameters=policy_model.parameters(),\n",
    ")\n",
    "reference_model, *_ = deepspeed.initialize(\n",
    "    model=reference_model,\n",
    "    config=ref_deepspeed_config,\n",
    ")\n",
    "\n",
    "reference_model.module.cpu()\n",
    "\n",
    "# Initialize SGLang (Inference) engine\n",
    "inference_engine = sglang.Engine(\n",
    "    model_path=MODEL_NAME,\n",
    "    enable_memory_saver=True,\n",
    "    skip_tokenizer_init=True,\n",
    "    mem_fraction_static=0.20,\n",
    "    schedule_policy=\"fcfs\",\n",
    "    schedule_conservativeness=0.001,\n",
    "    max_running_requests=10000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_test_compute_pg_loss():\n",
    "    reference_model.module.cuda()\n",
    "    test_case = {\n",
    "        \"labels\": torch.tensor([[0, 0, 0, 1, 1, 1, 0, 0]], device=\"cuda\"),\n",
    "        \"input_ids\": torch.tensor([[1, 2, 3, 4, 5, 6, 10, 10]], device=\"cuda\"),\n",
    "        \"advantages\": torch.tensor([[0.1, -0.5, +0.25, 0.3, 0.1, 0.1, 0.0, 0.0]], device=\"cuda\"),\n",
    "        \"attention_mask\": torch.tensor([[1, 1, 1, 1, 1, 1, 0, 0]], device=\"cuda\"),\n",
    "    }\n",
    "    \n",
    "    loss_ground, metrics_ground = compute_pg_loss_ground(policy_model=policy_model,\n",
    "                                                         reference_model=reference_model,\n",
    "                                                         batch=test_case,\n",
    "                                                         total_response_len=8,\n",
    "                                                         KL_COEFFICIENT=KL_COEFFICIENT,\n",
    "                                                         EOS_TOKEN=EOS_TOKEN,\n",
    "                                                         EOS_TOKEN_ID=EOS_TOKEN_ID,\n",
    "                                                         GENERATIONS_PER_SAMPLE=GENERATIONS_PER_SAMPLE,\n",
    "                                                         tokenizer=tokenizer,\n",
    "                                                         TEMPERATURE=TEMPERATURE)\n",
    "    loss_ground = loss_ground.item()\n",
    "    \n",
    "    loss, metrics = compute_pg_loss(policy_model, reference_model, test_case, total_response_len=8)\n",
    "    loss = loss.item()\n",
    "\n",
    "    assert np.abs(loss_ground - loss) < 1e-6, f\"Mismatch in loss: {loss_ground} != {loss}\"\n",
    "    for k, v in metrics_ground.items():\n",
    "        assert np.abs(metrics_ground[k] - metrics[k]) < 1e-6, f\"Mismatch in {k}: {metrics_ground[k]} != {metrics[k]}\"\n",
    "    \n",
    "unit_test_compute_pg_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_model.module.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb for logging\n",
    "wandb.init(\n",
    "    project=\"r1-aha-moment\",\n",
    "    name=RUN_NAME,\n",
    "    config={\n",
    "        \"model_name\": MODEL_NAME,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"num_iterations\": NUM_ITERATIONS,\n",
    "        \"episodes_per_iteration\": EPISODES_PER_ITERATION,\n",
    "        \"rollouts_per_episode\": GENERATIONS_PER_SAMPLE,\n",
    "        \"kl_coefficient\": KL_COEFFICIENT,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in trange(NUM_ITERATIONS):\n",
    "    print(f\"Iteration {iteration}/{NUM_ITERATIONS}\")\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    #########################################################\n",
    "    # Generate Episodes\n",
    "    #########################################################\n",
    "\n",
    "    # Sample training batch\n",
    "    num_samples = EPISODES_PER_ITERATION // GENERATIONS_PER_SAMPLE\n",
    "    indices = np.random.choice(\n",
    "        len(train_dataset), size=num_samples, replace=False\n",
    "    )\n",
    "    samples = train_dataset.select(indices)\n",
    "\n",
    "    # Update model weights in SGLang engine\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "    eval_stats = None\n",
    "    if iteration % 25 == 0 and False:\n",
    "        print(\"Evaluating on test set...\")\n",
    "        eval_stats = evaluate_on_test_set(\n",
    "            inference_engine=inference_engine,\n",
    "            test_dataset=test_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "            eos_token=EOS_TOKEN,\n",
    "            eval_sampling_params={\n",
    "                \"temperature\": 0.3,\n",
    "                \"max_new_tokens\": 1024,\n",
    "                \"n\": 1,\n",
    "            },\n",
    "            reward_func=compute_reward,\n",
    "        )\n",
    "        time.sleep(2)  # so sglang scheduler cools down\n",
    "\n",
    "    # Sample responses\n",
    "    outputs = inference_engine.generate(\n",
    "        input_ids=samples[\"input_ids\"], \n",
    "        sampling_params={\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"top_p\": TOP_P,\n",
    "            \"top_k\": TOP_K,\n",
    "            \"max_new_tokens\": MAX_RESPONSE_TOKENS,\n",
    "            \"n\": GENERATIONS_PER_SAMPLE,\n",
    "        }\n",
    "    )\n",
    "    all_generations = [o[\"token_ids\"] for o in outputs]\n",
    "    all_finish_reasons = [o[\"meta_info\"][\"finish_reason\"][\"type\"] for o in outputs]\n",
    "    inference_engine.release_memory_occupation()\n",
    "    \n",
    "    print(f\"Generated {len(all_generations)} responses\")\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Process responses and calculate rewards\n",
    "    episodes, episodes_stats = create_training_episodes(\n",
    "        samples,\n",
    "        all_generations,\n",
    "        all_finish_reasons,\n",
    "    )\n",
    "    for k, v in episodes_stats.items():\n",
    "        metrics.setdefault(k, []).extend(v)\n",
    "\n",
    "    #########################################################\n",
    "    # Training\n",
    "    #########################################################\n",
    "\n",
    "    # Prepare training batch\n",
    "    model_inputs = prepare_model_inputs(\n",
    "        query_token_ids=episodes[\"all_query_token_ids\"],\n",
    "        response_token_ids=episodes[\"all_response_token_ids\"],\n",
    "        advantages=episodes[\"all_advantages\"],\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    # Calculate losses and update model\n",
    "    policy_model.train()\n",
    "    reference_model.module.cuda()\n",
    "    reference_model.eval()\n",
    "\n",
    "    total_response_len = (model_inputs[\"labels\"] != -100).sum().item()\n",
    "\n",
    "    for i in trange(0, EPISODES_PER_ITERATION, PER_DEVICE_BATCH_SIZE):\n",
    "        print(f\"Processing batch {i}/{EPISODES_PER_ITERATION}\")\n",
    "        batch = {\n",
    "            k: v[i : i + PER_DEVICE_BATCH_SIZE]\n",
    "            for k, v in model_inputs.items()\n",
    "        }\n",
    "\n",
    "        # Compute policy gradient loss\n",
    "        loss, loss_metrics = compute_pg_loss(\n",
    "            policy_model=policy_model,\n",
    "            reference_model=reference_model,\n",
    "            batch=batch,\n",
    "            total_response_len=total_response_len,\n",
    "        )\n",
    "\n",
    "        print(loss_metrics[\"kl_penalty\"])\n",
    "\n",
    "        # Track metrics\n",
    "        metrics.setdefault(\"loss\", []).append(loss.item())\n",
    "        grad_norm = policy_model.get_global_grad_norm()\n",
    "        if grad_norm is not None:\n",
    "            grad_norm = grad_norm.item()\n",
    "        metrics.setdefault(\"grad_norm\", []).append(grad_norm)\n",
    "        for k, v in loss_metrics.items():\n",
    "            metrics.setdefault(k, []).append(v.item() if isinstance(v, torch.Tensor) else v)\n",
    "\n",
    "        # Backpropagation and optimization step\n",
    "        policy_model.backward(loss, scale_wrt_gas=False)\n",
    "        \n",
    "        # Free memory\n",
    "        del loss, loss_metrics\n",
    "        if policy_model.is_gradient_accumulation_boundary():\n",
    "            reference_model.module.cpu()\n",
    "\n",
    "        policy_model.step()\n",
    "\n",
    "    print(\"Finished training\")\n",
    "\n",
    "    #########################################################\n",
    "    # Update inference engine weights\n",
    "    #########################################################\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    time.sleep(1)\n",
    "\n",
    "    inference_engine.resume_memory_occupation()\n",
    "    success, error = inference_engine.update_weights_from_tensor(\n",
    "        list(policy_model.module.named_parameters())\n",
    "    )\n",
    "    if not success:\n",
    "        raise RuntimeError(f\"Weight update failed: {error}\")\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "    # Log metrics\n",
    "    #########################################################\n",
    "\n",
    "    train_metrics = {\n",
    "        k: np.mean(v) for k, v in metrics.items() if None not in v\n",
    "    }\n",
    "    train_metrics[\"learning_rate\"] = policy_model.get_lr()[0]\n",
    "    logs = {\n",
    "        \"iteration\": iteration,\n",
    "        **{f\"train/{k}\": v for k, v in train_metrics.items()},\n",
    "    }\n",
    "    if eval_stats is not None:\n",
    "        logs.update({f\"eval/{k}\": v for k, v in eval_stats.items()})\n",
    "    wandb.log(logs)\n",
    "\n",
    "    selected_keys = [\n",
    "        \"train/kl_penalty\",\n",
    "        \"train/rewards\",\n",
    "        \"train/reward_metrics/format_reward\",\n",
    "        \"train/reward_metrics/equation_reward\",\n",
    "        \"eval/rewards\",\n",
    "        \"eval/reward_metrics/format_reward\",\n",
    "        \"eval/reward_metrics/equation_reward\",\n",
    "    ]\n",
    "    selected_metrics = {k: logs[k] for k in selected_keys if k in logs}\n",
    "    print(f\"KEY METRICS: {selected_metrics}\")\n",
    "\n",
    "    if iteration % 50 == 0:\n",
    "        policy_model.module.save_pretrained(\n",
    "            str(EXP_DIR / \"checkpoints\" / f\"ckpt_{iteration:06d}\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 fixed modules",
   "language": "python",
   "name": "python3_fixed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
